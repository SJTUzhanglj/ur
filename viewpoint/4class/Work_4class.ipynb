{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# License: BSD\n",
    "# Author: Sasank Chilamkurthy+Zhang Liangjun\n",
    "#verson 1.0\n",
    "#class 2classes\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import scipy.io as scio\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import torch.utils.data as data\n",
    "import random\n",
    "import shutil\n",
    "import glob\n",
    "import csv\n",
    "import math\n",
    "import pandas\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#input:mat file\n",
    "#output: tensor\n",
    "def mat_read(filepath):\n",
    "    dataFile = filepath\n",
    "    data = scio.loadmat(dataFile)\n",
    "    #读取mat里的depth数据\n",
    "    depth = data['depth']\n",
    "    #归一化\n",
    "    depth=depth*300\n",
    "    depth=depth.astype(np.int)\n",
    "    depth=depth.astype(np.float32)\n",
    "    depth_scale=depth[depth>0]\n",
    "    avrg=np.mean(depth_scale)\n",
    "    var=np.std(depth_scale)\n",
    "    index=depth==0\n",
    "    depth[index]=avrg\n",
    "    depth_scale=(depth-avrg)/var\n",
    "    depth=np.array(depth_scale)\n",
    "    depth=np.expand_dims(depth,0)\n",
    "    return depth\n",
    "#把数据文件分成训练集和测试集\n",
    "def generate_datasets(data_dir, dst_dir, train_ratio=0.7):\n",
    "    train_dir = os.path.join(dst_dir,'train')\n",
    "    val_dir = os.path.join(dst_dir,'val')\n",
    "    if not os.path.exists(dst_dir):\n",
    "        os.mkdir(dst_dir)\n",
    "        os.mkdir(train_dir)\n",
    "        os.mkdir(val_dir)\n",
    "    for root, dirnames, _ in os.walk(data_dir):\n",
    "        for dirname in dirnames:\n",
    "            #因为需要的数据在更下一层的depth文件里，所以加了几句，后边带俩#的为改过的部分，\n",
    "            #如果不需要做depth改过来就行了\n",
    "            #depth_dirname=os.path.join(dirname,'depth')##\n",
    "            subdirname_train = os.path.join(train_dir, dirname)\n",
    "            subdirname_val = os.path.join(val_dir, dirname)\n",
    "            if not os.path.exists(subdirname_train):\n",
    "                os.mkdir(subdirname_train)\n",
    "            if not os.path.exists(subdirname_val):\n",
    "                os.mkdir(subdirname_val)                                        \n",
    "            #dname = os.path.join(root, depth_dirname)##depth_dirname原本是dirname\n",
    "            dname = os.path.join(root, dirname)\n",
    "            names = glob.glob(dname+r'/*.mat')  \n",
    "            random.shuffle(names)\n",
    "            names_len = len(names)\n",
    "            train_names = names[:int(names_len*train_ratio)]\n",
    "            val_names = names[int(names_len*train_ratio)+1:]\n",
    "            for f in train_names:\n",
    "                fname = os.path.split(f)[-1]\n",
    "                train_dname = os.path.join(subdirname_train, fname)\n",
    "                shutil.copyfile(f, train_dname)\n",
    "            for f in val_names:\n",
    "                fname = os.path.split(f)[-1]\n",
    "                val_dname = os.path.join(subdirname_val, fname)\n",
    "                shutil.copyfile(f, val_dname)\n",
    "            print ('copy {} done'.format(dname))\n",
    "        \n",
    "data_dir = '/home/sjtu/gcj/data/depth_data/depthmat2'\n",
    "dst_dir = '/home/sjtu/gcj/data/depth_data/depth_4class'\n",
    "#generate_datasets(data_dir, dst_dir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EXTENSIONS = ['.mat']\n",
    "def is_mat_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in EXTENSIONS)\n",
    "\n",
    "#类名\n",
    "def find_classes(dir):\n",
    "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir,d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]:i for i in range(len(classes))}\n",
    "    return classes,class_to_idx\n",
    "\n",
    "#input: dir+train(or val)+class\n",
    "#output: 数据文件的集合\n",
    "def make_dataset(dir,phase,class_to_idx):\n",
    "    datas = []\n",
    "    labels = []\n",
    "    dir = os.path.join(dir,phase)\n",
    "    for target in os.listdir(dir):\n",
    "        d = os.path.join(dir,target)\n",
    "        if not os.path.isdir(d):\n",
    "            continue\n",
    "        \n",
    "        for root, _, fnames in sorted(os.walk(d)):\n",
    "            for fname in fnames:\n",
    "                if is_mat_file(fname):\n",
    "                    path = os.path.join(root,fname)\n",
    "                    #depth=mat_read(path)\n",
    "                    item = (path,class_to_idx[target])\n",
    "                    datas.append(item)\n",
    "                    #datas.append(depth)\n",
    "                    #labels.append(class_to_idx[target])\n",
    "    return datas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ViewpointDataset(data.Dataset):\n",
    "    def __init__(self, root, transform = None, phase = None):\n",
    "        dir = os.path.join(root, phase)\n",
    "        classes, class_to_idx = find_classes(dir)\n",
    "        datas= make_dataset(root,phase, class_to_idx)\n",
    "        if len(datas) == 0:\n",
    "            raise(RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\"\n",
    "                               \"Supported extensions are: \" + \",\".join(EXTENSIONS)))\n",
    "        self.root = root\n",
    "        self.phase = phase\n",
    "        self.classes = classes\n",
    "        #todo\n",
    "        self.width = 480\n",
    "        self.height = 640\n",
    "        self.suffix = '.mat'\n",
    "        self.transform = transform\n",
    "        self.datas=datas\n",
    "        \n",
    "    #深度矩阵转成tensor  \n",
    "    def __getitem__(self, idx):\n",
    "        mat_path, label = self.datas[idx]\n",
    "        #preprocess\n",
    "        depth= mat_read(mat_path)\n",
    "        #depth,label=self.datas[idx]\n",
    "        #depth=self.datas[idx]\n",
    "        #label=self.labels[idx]\n",
    "\n",
    "\n",
    "        #create tensor from numpy.ndarray\n",
    "        depth=torch.from_numpy(depth)\n",
    "\n",
    "        depth_tensor=depth.type(torch.FloatTensor)\n",
    "        if self.transform:\n",
    "            toPIL=transforms.ToPILImage()\n",
    "            toTensor=transforms.ToTensor()\n",
    "            depth_tensor = toTensor(self.transform(toPIL(depth_tensor)))\n",
    "        return depth_tensor, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "    #设置梯度更新方式\n",
    "    #增大学习率是个好办法\n",
    "def optim_scheduler_ft(model, epoch, init_lr=0.005, lr_decay_epoch=7):\n",
    "    lr = init_lr * (0.1**(epoch // lr_decay_epoch))\n",
    "\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('LR is set to {}'.format(lr))\n",
    "\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 7168, 'val': 3064}\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#建立数据集\n",
    "#transform=transforms.Compose([transforms.RandomCrop([300,300])])\n",
    "transform=transforms.RandomCrop([300,300])\n",
    "#dsets = {x: ViewpointDataset(dst_dir,transform=transform,phase=x) for x in ['train', 'val']}\n",
    "dsets = {x: ViewpointDataset(dst_dir,phase=x) for x in ['train', 'val']}\n",
    "dset_loaders = {x:torch.utils.data.DataLoader(dsets[x],batch_size=24,shuffle=True, num_workers=8) for x in ['train', 'val']}\n",
    "dset_sizes = {x: len(dsets[x]) for x in ['train', 'val']}\n",
    "dset_classes = dsets['val'].classes\n",
    "print(dset_sizes)\n",
    "print(len(dset_classes))\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "  (conv2): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear (5120 -> 120)\n",
      "  (fc2): Linear (120 -> 84)\n",
      "  (fc3): Linear (84 -> 4)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\na=mat_read('/home/sjtu/gcj/data/depth_data/depth_2class/train/off_32_0_180/95_depth.mat')\\ndepth=torch.from_numpy(a)\\na=depth.type(torch.FloatTensor)\\nc=torch.FloatTensor(1,1,480,640)#\\xe9\\x9c\\x80\\xe8\\xa6\\x81\\xe5\\x86\\x99\\xe6\\x88\\x903dtensor 1\\xe4\\xbb\\xa3\\xe8\\xa1\\xa8batch\\nc[0]=a\\nb=Variable(c.cuda())\\nprint(type(b))\\noutputs = model(b)\\nprint outputs\\n#\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#训练所用网络模型：\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(11,11),stride=(4,4),padding=(2,2)) # 1 input image channel, 6 output channels, 5x5 square convolution kernel\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "        #self.conv3 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        #480*640:5120  300*300:1024\n",
    "        self.fc1   = nn.Linear(5120, 120) # an affine operation: y = Wx + b\n",
    "        self.fc2   = nn.Linear(120,84)\n",
    "        #self.fc2   = nn.Linear(1024, 120)\n",
    "        self.fc3   = nn.Linear(84, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (3,3)) # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 3) # If the size is a square you can only specify a single number\n",
    "        #x = F.max_pool2d(F.relu(self.conv3(x)), 3) \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv5(x)), 3) \n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        #'''\n",
    "        x = F.dropout(x,p=0.5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x=  F.dropout(x,p=0.5)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #x=  F.dropout(x,p=0.5)\n",
    "        #x = F.relu(self.fc3(x))\n",
    "        x = self.fc3(x)\n",
    "        #'''\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "model = Net().cuda()\n",
    "print(model)\n",
    "'''\n",
    "a=mat_read('/home/sjtu/gcj/data/depth_data/depth_2class/train/off_32_0_180/95_depth.mat')\n",
    "depth=torch.from_numpy(a)\n",
    "a=depth.type(torch.FloatTensor)\n",
    "c=torch.FloatTensor(1,1,480,640)#需要写成3dtensor 1代表batch\n",
    "c[0]=a\n",
    "b=Variable(c.cuda())\n",
    "print(type(b))\n",
    "outputs = model(b)\n",
    "print outputs\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##定义模型如何训练\n",
    "def train_model(model, criterion, optim_scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "        \n",
    "    best_model = model\n",
    "    best_acc = 0.0\n",
    "    count=0\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                optimizer = optim_scheduler(model, epoch)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in dset_loaders[phase]:\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "                #print(type(inputs))\n",
    "                # wrap them in Variable\n",
    "                if use_gpu and phase=='train':\n",
    "                    inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "                elif use_gpu and phase=='val':\n",
    "                    inputs, labels = Variable(inputs.cuda(),volatile=True), Variable(labels.cuda(),volatile=True)\n",
    "                    #inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "                #print(type(inputs))\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.data[0]\n",
    "                running_corrects += torch.sum(preds== labels.data)\n",
    "                #print(running_loss)\n",
    "\n",
    "            epoch_loss = running_loss / dset_sizes[phase]\n",
    "            epoch_acc = float(running_corrects) / float(dset_sizes[phase])\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc >= best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model = copy.deepcopy(model)\n",
    "        model_name='model_4class_'+str(count)+'.pth'\n",
    "        torch.save(best_model,model_name)\n",
    "        count=count+1\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    torch.save(best_model, 'model_2class.pth')\n",
    "    print('done')\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "LR is set to 0.005\n",
      "train Loss: 0.0155 Acc: 0.8807\n",
      "val Loss: 0.0037 Acc: 0.9723\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.0059 Acc: 0.9681\n",
      "val Loss: 0.0009 Acc: 0.9931\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.0063 Acc: 0.9760\n",
      "val Loss: 0.0016 Acc: 0.9935\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.0063 Acc: 0.9768\n",
      "val Loss: 0.0051 Acc: 0.9791\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.0080 Acc: 0.9739\n",
      "val Loss: 0.0171 Acc: 0.8841\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.0119 Acc: 0.9713\n",
      "val Loss: 0.0023 Acc: 0.9925\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.0086 Acc: 0.9686\n",
      "val Loss: 0.0040 Acc: 0.9886\n",
      "Epoch 7/24\n",
      "----------\n",
      "LR is set to 0.0005\n",
      "train Loss: 0.0009 Acc: 0.9950\n",
      "val Loss: 0.0012 Acc: 0.9964\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9979\n",
      "val Loss: 0.0011 Acc: 0.9980\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 0.9993\n",
      "val Loss: 0.0008 Acc: 0.9984\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9994\n",
      "val Loss: 0.0013 Acc: 0.9990\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9996\n",
      "val Loss: 0.0008 Acc: 0.9993\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0012 Acc: 0.9990\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9990\n",
      "val Loss: 0.0020 Acc: 0.9987\n",
      "Epoch 14/24\n",
      "----------\n",
      "LR is set to 5e-05\n",
      "train Loss: 0.0001 Acc: 0.9999\n",
      "val Loss: 0.0015 Acc: 0.9987\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0013 Acc: 0.9990\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0011 Acc: 0.9990\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0012 Acc: 0.9987\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0012 Acc: 0.9987\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0012 Acc: 0.9987\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0012 Acc: 0.9987\n",
      "Epoch 21/24\n",
      "----------\n",
      "LR is set to 5e-06\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0012 Acc: 0.9987\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0012 Acc: 0.9987\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0012 Acc: 0.9987\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0012 Acc: 0.9987\n",
      "Training complete in 10m 4s\n",
      "Best val Acc: 0.999347\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.MSELoss()\n",
    "model = train_model(model,criterion, optim_scheduler_ft, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
